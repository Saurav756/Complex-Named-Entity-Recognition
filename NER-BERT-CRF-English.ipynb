{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate seqeval\n",
    "!git clone https://github.com/kmkurn/pytorch-crf.git\n",
    "!pip install ./pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, DataCollatorForTokenClassification, TrainingArguments, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from torchcrf import CRF\n",
    "from seqeval.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the CoNLL-2003 dataset\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "# Reduce the dataset size (30% of the original size) for quicker experimentation\n",
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(int(len(dataset[\"train\"]))))\n",
    "dataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=42).select(range(int(len(dataset[\"validation\"]))))\n",
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=42).select(range(int(len(dataset[\"test\"]))))\n",
    "\n",
    "# Get label names\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names  # e.g., O, B-PER, I-PER\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"bert-large-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization and label alignment\n",
    "def tokenize_and_align_labels_with_crf(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Ignored during training\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])  # First subword\n",
    "            else:\n",
    "                label_ids.append(-100)  # Ignore subwords\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels_with_crf, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].remove_columns([\"id\", \"pos_tags\", \"chunk_tags\", \"tokens\"])\n",
    "tokenized_datasets[\"validation\"] = tokenized_datasets[\"validation\"].remove_columns([\"id\", \"pos_tags\", \"chunk_tags\", \"tokens\"])\n",
    "tokenized_datasets[\"test\"] = tokenized_datasets[\"test\"].remove_columns([\"id\", \"pos_tags\", \"chunk_tags\", \"tokens\"])\n",
    "\n",
    "# Set dataset format\n",
    "tokenized_datasets[\"train\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_datasets[\"validation\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_datasets[\"test\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# DataLoader setup\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=16, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=16, collate_fn=data_collator)\n",
    "\n",
    "# Define the model with CRF\n",
    "class BertCRFNER(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(BertCRFNER, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(self.dropout(outputs.last_hidden_state))  # Apply dropout\n",
    "\n",
    "        if labels is not None:\n",
    "            # Replace -100 with a valid index (e.g., 0)\n",
    "            valid_labels = labels.clone()\n",
    "            valid_labels[labels == -100] = 0\n",
    "\n",
    "            # Compute CRF loss\n",
    "            loss = -self.crf(logits, valid_labels, mask=attention_mask.bool())\n",
    "            return loss\n",
    "        else:\n",
    "            # Decode CRF predictions\n",
    "            predictions = self.crf.decode(logits, mask=attention_mask.bool())\n",
    "            return predictions\n",
    "\n",
    "# Initialize TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate once per epoch\n",
    "    save_strategy=\"epoch\",  # Save only at the end of each epoch\n",
    "    save_total_limit=1,  # Keep only the latest checkpoint\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=None,  # Disable logging directory\n",
    "    logging_steps=50,  # Log only every 50 steps\n",
    "    report_to=\"none\",  # Disable reporting\n",
    "    save_on_each_node=False,  # Avoid saving duplicates in distributed training\n",
    ")\n",
    "\n",
    "# Extract Parameters\n",
    "learning_rate = training_args.learning_rate\n",
    "weight_decay = training_args.weight_decay\n",
    "num_epochs = training_args.num_train_epochs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "output_dir = training_args.output_dir\n",
    "\n",
    "# Initialize the model\n",
    "improved_model = BertCRFNER(model_name=model_name, num_labels=num_labels)\n",
    "improved_model.to(device)\n",
    "\n",
    "# Freeze lower layers for initial training\n",
    "for param in improved_model.bert.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for layer in improved_model.bert.encoder.layer[:6]:  # Freeze first 6 layers\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(improved_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training and Validation Loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    improved_model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = improved_model(input_ids, attention_mask, labels=labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(improved_model.parameters(), max_norm=1.0)  # Optional gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}: Training Loss = {avg_train_loss}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    improved_model.eval()\n",
    "    val_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            loss = improved_model(input_ids, attention_mask, labels=labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = improved_model(input_ids, attention_mask)\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}: Validation Loss = {avg_val_loss}\")\n",
    "    decoded_predictions = []\n",
    "    decoded_labels = []\n",
    "    for preds, labels in zip(predictions, true_labels):\n",
    "        valid_preds = [p for p, l in zip(preds, labels) if l != -100]\n",
    "        valid_labels = [l for l in labels if l != -100]\n",
    "        decoded_predictions.append([label_list[p] for p in valid_preds])\n",
    "        decoded_labels.append([label_list[l] for l in valid_labels])\n",
    "\n",
    "    print(f\"Classification Report for Epoch {epoch + 1}:\")\n",
    "    print(classification_report(decoded_labels, decoded_predictions))\n",
    "\n",
    "    # Save Model at the End of Each Epoch\n",
    "    if training_args.save_strategy == \"epoch\":\n",
    "        save_path = f\"{output_dir}/epoch-{epoch + 1}\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # Save the model's state dictionary\n",
    "        model_save_path = os.path.join(save_path, \"pytorch_model.bin\")\n",
    "        torch.save(improved_model.state_dict(), model_save_path)\n",
    "\n",
    "        # Save the tokenizer\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "\n",
    "        print(f\"Model and tokenizer saved at {save_path}\")\n",
    "\n",
    "# Testing the Model\n",
    "print(\"Starting Testing Phase...\")\n",
    "improved_model.eval()\n",
    "test_predictions, test_labels = [], []\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Compute loss for testing\n",
    "        loss = improved_model(input_ids, attention_mask, labels=labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Get predictions\n",
    "        preds = improved_model(input_ids, attention_mask)\n",
    "        test_predictions.extend(preds)\n",
    "        test_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "print(f\"Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# Decode predictions and labels\n",
    "decoded_test_predictions = []\n",
    "decoded_test_labels = []\n",
    "for preds, labels in zip(test_predictions, test_labels):\n",
    "    valid_preds = [p for p, l in zip(preds, labels) if l != -100]\n",
    "    valid_labels = [l for l in labels if l != -100]\n",
    "    decoded_test_predictions.append([label_list[p] for p in valid_preds])\n",
    "    decoded_test_labels.append([label_list[l] for l in valid_labels])\n",
    "\n",
    "# Classification Report\n",
    "print(\"Test Set Classification Report:\")\n",
    "print(classification_report(decoded_test_labels, decoded_test_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using device: cuda\n",
    "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
    "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
    "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
    "You will be able to reuse this secret in all of your notebooks.\n",
    "Please note that authentication is recommended but still optional to access public models or datasets.\n",
    "  warnings.warn(\n",
    "The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003.\n",
    "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
    "\n",
    "Do you wish to run the custom code? [y/N] y\n",
    "Downloading data: 100%\n",
    " 983k/983k [00:00<00:00, 20.4MB/s]\n",
    "Generating train split: 100%\n",
    " 14041/14041 [00:05<00:00, 2489.13 examples/s]\n",
    "Generating validation split: 100%\n",
    " 3250/3250 [00:01<00:00, 3003.64 examples/s]\n",
    "Generating test split: 100%\n",
    " 3453/3453 [00:00<00:00, 4578.28 examples/s]\n",
    "tokenizer_config.json: 100%\n",
    " 49.0/49.0 [00:00<00:00, 2.46kB/s]\n",
    "config.json: 100%\n",
    " 762/762 [00:00<00:00, 58.2kB/s]\n",
    "vocab.txt: 100%\n",
    " 213k/213k [00:00<00:00, 865kB/s]\n",
    "tokenizer.json: 100%\n",
    " 436k/436k [00:00<00:00, 1.74MB/s]\n",
    "Map: 100%\n",
    " 14041/14041 [00:04<00:00, 2904.87 examples/s]\n",
    "Map: 100%\n",
    " 3250/3250 [00:00<00:00, 3545.38 examples/s]\n",
    "Map: 100%\n",
    " 3453/3453 [00:01<00:00, 2598.57 examples/s]\n",
    "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
    "  warnings.warn(\n",
    "model.safetensors: 100%\n",
    " 1.34G/1.34G [00:11<00:00, 152MB/s]\n",
    "Training Epoch 1: 100%|██████████| 878/878 [15:21<00:00,  1.05s/it]\n",
    "Epoch 1: Training Loss = 23.584132590978182\n",
    "Evaluating: 100%|██████████| 204/204 [02:45<00:00,  1.23it/s]\n",
    "Epoch 1: Validation Loss = 9.506981639301076\n",
    "Classification Report for Epoch 1:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.96      0.97      0.97      1837\n",
    "        MISC       0.86      0.89      0.87       922\n",
    "         ORG       0.93      0.94      0.93      1341\n",
    "         PER       0.98      0.98      0.98      1836\n",
    "\n",
    "   micro avg       0.94      0.95      0.95      5936\n",
    "   macro avg       0.93      0.94      0.94      5936\n",
    "weighted avg       0.94      0.95      0.95      5936\n",
    "\n",
    "Model and tokenizer saved at ./results/epoch-1\n",
    "Training Epoch 2: 100%|██████████| 878/878 [15:28<00:00,  1.06s/it]\n",
    "Epoch 2: Training Loss = 8.143380171617235\n",
    "Evaluating: 100%|██████████| 204/204 [02:45<00:00,  1.23it/s]\n",
    "Epoch 2: Validation Loss = 11.9276491800944\n",
    "Classification Report for Epoch 2:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.97      0.96      0.97      1837\n",
    "        MISC       0.88      0.91      0.89       922\n",
    "         ORG       0.91      0.94      0.93      1341\n",
    "         PER       0.98      0.97      0.98      1836\n",
    "\n",
    "   micro avg       0.94      0.95      0.95      5936\n",
    "   macro avg       0.93      0.95      0.94      5936\n",
    "weighted avg       0.95      0.95      0.95      5936\n",
    "\n",
    "Model and tokenizer saved at ./results/epoch-2\n",
    "Training Epoch 3: 100%|██████████| 878/878 [15:25<00:00,  1.05s/it]\n",
    "Epoch 3: Training Loss = 5.313750869863941\n",
    "Evaluating: 100%|██████████| 204/204 [02:45<00:00,  1.23it/s]\n",
    "Epoch 3: Validation Loss = 14.283800209269804\n",
    "Classification Report for Epoch 3:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.97      0.97      0.97      1837\n",
    "        MISC       0.92      0.92      0.92       922\n",
    "         ORG       0.94      0.94      0.94      1341\n",
    "         PER       0.98      0.98      0.98      1836\n",
    "\n",
    "   micro avg       0.96      0.96      0.96      5936\n",
    "   macro avg       0.95      0.96      0.95      5936\n",
    "weighted avg       0.96      0.96      0.96      5936\n",
    "\n",
    "Model and tokenizer saved at ./results/epoch-3\n",
    "Training Epoch 4: 100%|██████████| 878/878 [15:25<00:00,  1.05s/it]\n",
    "Epoch 4: Training Loss = 3.1521896631679667\n",
    "Evaluating: 100%|██████████| 204/204 [02:45<00:00,  1.23it/s]\n",
    "Epoch 4: Validation Loss = 14.71805798773672\n",
    "Classification Report for Epoch 4:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.98      0.97      0.98      1837\n",
    "        MISC       0.91      0.93      0.92       922\n",
    "         ORG       0.94      0.95      0.94      1341\n",
    "         PER       0.97      0.99      0.98      1836\n",
    "\n",
    "   micro avg       0.96      0.96      0.96      5936\n",
    "   macro avg       0.95      0.96      0.95      5936\n",
    "weighted avg       0.96      0.96      0.96      5936\n",
    "\n",
    "Model and tokenizer saved at ./results/epoch-4\n",
    "Training Epoch 5: 100%|██████████| 878/878 [15:23<00:00,  1.05s/it]\n",
    "Epoch 5: Training Loss = 1.9186030715907625\n",
    "Evaluating: 100%|██████████| 204/204 [02:45<00:00,  1.23it/s]\n",
    "Epoch 5: Validation Loss = 15.730226301679425\n",
    "Classification Report for Epoch 5:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.97      0.97      0.97      1837\n",
    "        MISC       0.91      0.93      0.92       922\n",
    "         ORG       0.93      0.95      0.94      1341\n",
    "         PER       0.98      0.98      0.98      1836\n",
    "\n",
    "   micro avg       0.96      0.96      0.96      5936\n",
    "   macro avg       0.95      0.96      0.95      5936\n",
    "weighted avg       0.96      0.96      0.96      5936\n",
    "\n",
    "Model and tokenizer saved at ./results/epoch-5\n",
    "Starting Testing Phase...\n",
    "Testing: 100%|██████████| 216/216 [02:55<00:00,  1.23it/s]\n",
    "Test Loss: 36.4662637533965\n",
    "Test Set Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.93      0.93      0.93      1666\n",
    "        MISC       0.79      0.83      0.81       702\n",
    "         ORG       0.88      0.91      0.90      1661\n",
    "         PER       0.97      0.97      0.97      1615\n",
    "\n",
    "   micro avg       0.91      0.92      0.92      5644\n",
    "   macro avg       0.89      0.91      0.90      5644\n",
    "weighted avg       0.91      0.92      0.92      5644\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
