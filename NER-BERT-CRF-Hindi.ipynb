{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate seqeval\n",
    "!git clone https://github.com/kmkurn/pytorch-crf.git\n",
    "!pip install ./pytorch-crf\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikiann\", \"hi\")  # Example: Hindi WikiANN dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the save directory in Google Drive\n",
    "drive_save_path = '/content/drive/My Drive/bert_crf_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, DataCollatorForTokenClassification, TrainingArguments, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from torchcrf import CRF\n",
    "from seqeval.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the CoNLL-2003 dataset\n",
    "dataset = load_dataset(\"wikiann\", \"hi\")  # Example: Hindi WikiANN dataset\n",
    "\n",
    "# Reduce the dataset size (30% of the original size) for quicker experimentation\n",
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(int(len(dataset[\"train\"]))))\n",
    "dataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=42).select(range(int(len(dataset[\"validation\"]))))\n",
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=42).select(range(int(len(dataset[\"test\"]))))\n",
    "\n",
    "# Get label names\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names  # e.g., O, B-PER, I-PER\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"bert-large-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization and label alignment\n",
    "def tokenize_and_align_labels_with_crf(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Ignored during training\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])  # First subword\n",
    "            else:\n",
    "                label_ids.append(-100)  # Ignore subwords\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels_with_crf, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].remove_columns([\"langs\", \"spans\", \"tokens\"])\n",
    "tokenized_datasets[\"validation\"] = tokenized_datasets[\"validation\"].remove_columns([\"langs\", \"spans\", \"tokens\"])\n",
    "tokenized_datasets[\"test\"] = tokenized_datasets[\"test\"].remove_columns([\"langs\", \"spans\", \"tokens\"])\n",
    "\n",
    "# Set dataset format\n",
    "tokenized_datasets[\"train\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_datasets[\"validation\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_datasets[\"test\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# DataLoader setup\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=16, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=16, collate_fn=data_collator)\n",
    "\n",
    "# Define the model with CRF\n",
    "class BertCRFNER(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(BertCRFNER, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(self.dropout(outputs.last_hidden_state))  # Apply dropout\n",
    "\n",
    "        if labels is not None:\n",
    "            # Replace -100 with a valid index (e.g., 0)\n",
    "            valid_labels = labels.clone()\n",
    "            valid_labels[labels == -100] = 0\n",
    "\n",
    "            # Compute CRF loss\n",
    "            loss = -self.crf(logits, valid_labels, mask=attention_mask.bool())\n",
    "            return loss\n",
    "        else:\n",
    "            # Decode CRF predictions\n",
    "            predictions = self.crf.decode(logits, mask=attention_mask.bool())\n",
    "            return predictions\n",
    "\n",
    "# Initialize TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate once per epoch\n",
    "    save_strategy=\"epoch\",  # Save only at the end of each epoch\n",
    "    save_total_limit=1,  # Keep only the latest checkpoint\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=None,  # Disable logging directory\n",
    "    logging_steps=50,  # Log only every 50 steps\n",
    "    report_to=\"none\",  # Disable reporting\n",
    "    save_on_each_node=False,  # Avoid saving duplicates in distributed training\n",
    ")\n",
    "\n",
    "# Extract Parameters\n",
    "learning_rate = training_args.learning_rate\n",
    "weight_decay = training_args.weight_decay\n",
    "num_epochs = training_args.num_train_epochs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "output_dir = training_args.output_dir\n",
    "\n",
    "# Initialize the model\n",
    "improved_model = BertCRFNER(model_name=model_name, num_labels=num_labels)\n",
    "improved_model.to(device)\n",
    "\n",
    "# Freeze lower layers for initial training\n",
    "for param in improved_model.bert.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for layer in improved_model.bert.encoder.layer[:6]:  # Freeze first 6 layers\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(improved_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training and Validation Loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    improved_model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = improved_model(input_ids, attention_mask, labels=labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(improved_model.parameters(), max_norm=1.0)  # Optional gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}: Training Loss = {avg_train_loss}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    improved_model.eval()\n",
    "    val_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            loss = improved_model(input_ids, attention_mask, labels=labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = improved_model(input_ids, attention_mask)\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}: Validation Loss = {avg_val_loss}\")\n",
    "    decoded_predictions = []\n",
    "    decoded_labels = []\n",
    "    for preds, labels in zip(predictions, true_labels):\n",
    "        valid_preds = [p for p, l in zip(preds, labels) if l != -100]\n",
    "        valid_labels = [l for l in labels if l != -100]\n",
    "        decoded_predictions.append([label_list[p] for p in valid_preds])\n",
    "        decoded_labels.append([label_list[l] for l in valid_labels])\n",
    "\n",
    "    print(f\"Classification Report for Epoch {epoch + 1}:\")\n",
    "    print(classification_report(decoded_labels, decoded_predictions))\n",
    "\n",
    "     # Save Model and Tokenizer to Google Drive\n",
    "    if training_args.save_strategy == \"epoch\":\n",
    "        save_path = os.path.join(drive_save_path, f\"epoch-{epoch + 1}\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # Save the model's state dictionary\n",
    "        model_save_path = os.path.join(save_path, \"pytorch_model.bin\")\n",
    "        torch.save(improved_model.state_dict(), model_save_path)\n",
    "\n",
    "        # Save the tokenizer\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "\n",
    "        print(f\"Model and tokenizer saved at {save_path}\")\n",
    "\n",
    "# Testing the Model\n",
    "print(\"Starting Testing Phase...\")\n",
    "improved_model.eval()\n",
    "test_predictions, test_labels = [], []\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Compute loss for testing\n",
    "        loss = improved_model(input_ids, attention_mask, labels=labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Get predictions\n",
    "        preds = improved_model(input_ids, attention_mask)\n",
    "        test_predictions.extend(preds)\n",
    "        test_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "print(f\"Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# Decode predictions and labels\n",
    "decoded_test_predictions = []\n",
    "decoded_test_labels = []\n",
    "for preds, labels in zip(test_predictions, test_labels):\n",
    "    valid_preds = [p for p, l in zip(preds, labels) if l != -100]\n",
    "    valid_labels = [l for l in labels if l != -100]\n",
    "    decoded_test_predictions.append([label_list[p] for p in valid_preds])\n",
    "    decoded_test_labels.append([label_list[l] for l in valid_labels])\n",
    "\n",
    "# Classification Report\n",
    "print(\"Test Set Classification Report:\")\n",
    "print(classification_report(decoded_test_labels, decoded_test_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using device: cuda\n",
    "Map:â€‡100%\n",
    "â€‡1000/1000â€‡[00:00<00:00,â€‡4164.85â€‡examples/s]\n",
    "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
    "  warnings.warn(\n",
    "Training Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [05:23<00:00,  1.03s/it]\n",
    "Epoch 1: Training Loss = 98.74091459767887\n",
    "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
    "Epoch 1: Validation Loss = 61.85583732241676\n",
    "Classification Report for Epoch 1:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.50      0.38      0.43       423\n",
    "         ORG       0.33      0.62      0.43       369\n",
    "         PER       0.60      0.61      0.60       434\n",
    "\n",
    "   micro avg       0.45      0.53      0.48      1226\n",
    "   macro avg       0.47      0.53      0.49      1226\n",
    "weighted avg       0.48      0.53      0.49      1226\n",
    "\n",
    "Model and tokenizer saved at /content/drive/My Drive/bert_crf_model/epoch-1\n",
    "Training Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [05:29<00:00,  1.05s/it]\n",
    "Epoch 2: Training Loss = 58.32604721483712\n",
    "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.26it/s]\n",
    "Epoch 2: Validation Loss = 46.512420412093874\n",
    "Classification Report for Epoch 2:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.69      0.60      0.64       423\n",
    "         ORG       0.45      0.63      0.53       369\n",
    "         PER       0.68      0.77      0.72       434\n",
    "\n",
    "   micro avg       0.60      0.67      0.63      1226\n",
    "   macro avg       0.61      0.67      0.63      1226\n",
    "weighted avg       0.62      0.67      0.64      1226\n",
    "\n",
    "Model and tokenizer saved at /content/drive/My Drive/bert_crf_model/epoch-2\n",
    "Training Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [05:31<00:00,  1.06s/it]\n",
    "Epoch 3: Training Loss = 41.98486045374276\n",
    "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:51<00:00,  1.23it/s]\n",
    "Epoch 3: Validation Loss = 44.532665827917675\n",
    "Classification Report for Epoch 3:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.69      0.61      0.65       423\n",
    "         ORG       0.52      0.66      0.58       369\n",
    "         PER       0.72      0.80      0.75       434\n",
    "\n",
    "   micro avg       0.64      0.69      0.66      1226\n",
    "   macro avg       0.64      0.69      0.66      1226\n",
    "weighted avg       0.65      0.69      0.66      1226\n",
    "\n",
    "Model and tokenizer saved at /content/drive/My Drive/bert_crf_model/epoch-3\n",
    "Training Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [05:30<00:00,  1.06s/it]\n",
    "Epoch 4: Training Loss = 32.92744721848363\n",
    "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.26it/s]\n",
    "Epoch 4: Validation Loss = 43.91161467537047\n",
    "Classification Report for Epoch 4:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.61      0.71      0.66       423\n",
    "         ORG       0.58      0.57      0.57       369\n",
    "         PER       0.71      0.78      0.74       434\n",
    "\n",
    "   micro avg       0.64      0.69      0.66      1226\n",
    "   macro avg       0.63      0.69      0.66      1226\n",
    "weighted avg       0.64      0.69      0.66      1226\n",
    "\n",
    "Model and tokenizer saved at /content/drive/My Drive/bert_crf_model/epoch-4\n",
    "Training Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [05:40<00:00,  1.09s/it]\n",
    "Epoch 5: Training Loss = 24.613775880953757\n",
    "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
    "Epoch 5: Validation Loss = 46.61832667153979\n",
    "Classification Report for Epoch 5:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.73      0.70      0.71       423\n",
    "         ORG       0.65      0.70      0.68       369\n",
    "         PER       0.78      0.81      0.79       434\n",
    "\n",
    "   micro avg       0.72      0.74      0.73      1226\n",
    "   macro avg       0.72      0.74      0.73      1226\n",
    "weighted avg       0.72      0.74      0.73      1226\n",
    "\n",
    "Model and tokenizer saved at /content/drive/My Drive/bert_crf_model/epoch-5\n",
    "Training Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [05:30<00:00,  1.06s/it]\n",
    "Epoch 6: Training Loss = 19.596256414541422\n",
    "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:51<00:00,  1.23it/s]\n",
    "Epoch 6: Validation Loss = 47.64252913944305\n",
    "Classification Report for Epoch 6:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.67      0.75      0.71       423\n",
    "         ORG       0.63      0.72      0.67       369\n",
    "         PER       0.82      0.81      0.81       434\n",
    "\n",
    "   micro avg       0.70      0.76      0.73      1226\n",
    "   macro avg       0.71      0.76      0.73      1226\n",
    "weighted avg       0.71      0.76      0.73      1226\n",
    "\n",
    "Model and tokenizer saved at /content/drive/My Drive/bert_crf_model/epoch-6\n",
    "Training Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [05:33<00:00,  1.07s/it]\n",
    "Epoch 7: Training Loss = 15.189744675121368\n",
    "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
    "Epoch 7: Validation Loss = 51.164234645782955\n",
    "Classification Report for Epoch 7:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.66      0.75      0.70       423\n",
    "         ORG       0.70      0.70      0.70       369\n",
    "         PER       0.78      0.83      0.81       434\n",
    "\n",
    "   micro avg       0.71      0.77      0.74      1226\n",
    "   macro avg       0.71      0.76      0.74      1226\n",
    "weighted avg       0.72      0.77      0.74      1226\n",
    "\n",
    "Model and tokenizer saved at /content/drive/My Drive/bert_crf_model/epoch-7\n",
    "Training Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [05:28<00:00,  1.05s/it]\n",
    "Epoch 8: Training Loss = 12.168577663433819\n",
    "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:51<00:00,  1.21it/s]\n",
    "Epoch 8: Validation Loss = 54.782096590314595\n",
    "Classification Report for Epoch 8:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.75      0.73      0.74       423\n",
    "         ORG       0.64      0.69      0.66       369\n",
    "         PER       0.77      0.84      0.80       434\n",
    "\n",
    "   micro avg       0.72      0.76      0.74      1226\n",
    "   macro avg       0.72      0.75      0.73      1226\n",
    "weighted avg       0.72      0.76      0.74      1226\n",
    "\n",
    "Model and tokenizer saved at /content/drive/My Drive/bert_crf_model/epoch-8\n",
    "Training Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [05:28<00:00,  1.05s/it]\n",
    "Epoch 9: Training Loss = 10.0409609761101\n",
    "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
    "Epoch 9: Validation Loss = 54.340050803290474\n",
    "Classification Report for Epoch 9:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.74      0.77      0.75       423\n",
    "         ORG       0.71      0.69      0.70       369\n",
    "         PER       0.77      0.86      0.82       434\n",
    "\n",
    "   micro avg       0.74      0.78      0.76      1226\n",
    "   macro avg       0.74      0.77      0.76      1226\n",
    "weighted avg       0.74      0.78      0.76      1226\n",
    "\n",
    "Model and tokenizer saved at /content/drive/My Drive/bert_crf_model/epoch-9\n",
    "Training Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [05:46<00:00,  1.11s/it]\n",
    "Epoch 10: Training Loss = 8.320954709769056\n",
    "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:52<00:00,  1.20it/s]\n",
    "Epoch 10: Validation Loss = 56.66284682258727\n",
    "Classification Report for Epoch 10:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.74      0.77      0.75       423\n",
    "         ORG       0.70      0.74      0.72       369\n",
    "         PER       0.81      0.84      0.82       434\n",
    "\n",
    "   micro avg       0.75      0.78      0.77      1226\n",
    "   macro avg       0.75      0.78      0.77      1226\n",
    "weighted avg       0.75      0.78      0.77      1226\n",
    "\n",
    "Model and tokenizer saved at /content/drive/My Drive/bert_crf_model/epoch-10\n",
    "Starting Testing Phase...\n",
    "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]Test Loss: 60.79998403882224\n",
    "Test Set Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC       0.74      0.79      0.76       414\n",
    "         ORG       0.68      0.78      0.73       363\n",
    "         PER       0.80      0.77      0.79       450\n",
    "\n",
    "   micro avg       0.74      0.78      0.76      1227\n",
    "   macro avg       0.74      0.78      0.76      1227\n",
    "weighted avg       0.75      0.78      0.76      1227"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
