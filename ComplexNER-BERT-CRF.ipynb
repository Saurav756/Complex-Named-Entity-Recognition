{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/kmkurn/pytorch-crf.git\n",
    "!pip install ./pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets seqeval tqdm matplotlib\n",
    "!pip install 'transformers[sentencepiece]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, DataCollatorForTokenClassification, TrainingArguments, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from torchcrf import CRF\n",
    "from seqeval.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from google.colab import drive\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the directory to save checkpoints in Google Drive\n",
    "google_drive_dir = \"/content/drive/My Drive/NER_checkpoints\"\n",
    "os.makedirs(google_drive_dir, exist_ok=True)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# resume_from_checkpoint = \"/content/drive/My Drive/NER_checkpoints/latest_checkpoint.pt.gz\"\n",
    "resume_from_checkpoint = None\n",
    "start_epoch = 0\n",
    "\n",
    "# Load the CoNLL-2003 dataset\n",
    "dataset = load_dataset(\"MultiCoNER/multiconer_v2\", \"English (EN)\")\n",
    "\n",
    "# Reduce the dataset size (30% of the original size) for quicker experimentation\n",
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(int(len(dataset[\"train\"]))))\n",
    "dataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=42).select(range(int(len(dataset[\"validation\"]))))\n",
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=42).select(range(int(len(dataset[\"test\"]))))\n",
    "\n",
    "# Get label names\n",
    "label_list = dataset[\"train\"].features[\"ner_tags_index\"].feature.names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"bert-large-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization and label alignment function\n",
    "def tokenize_and_align_labels_with_crf(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags_index\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels_with_crf, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].remove_columns([\"id\", \"sample_id\", \"tokens\", \"ner_tags\", \"ner_tags_index\"])\n",
    "tokenized_datasets[\"validation\"] = tokenized_datasets[\"validation\"].remove_columns([\"id\", \"sample_id\", \"tokens\", \"ner_tags\", \"ner_tags_index\"])\n",
    "tokenized_datasets[\"test\"] = tokenized_datasets[\"test\"].remove_columns([\"id\", \"sample_id\", \"tokens\", \"ner_tags\", \"ner_tags_index\"])\n",
    "\n",
    "# Set dataset format\n",
    "tokenized_datasets[\"train\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_datasets[\"validation\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_datasets[\"test\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# DataLoader setup\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=16, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=16, collate_fn=data_collator)\n",
    "\n",
    "\n",
    "# Define the model with CRF\n",
    "class BertCRFNER(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(BertCRFNER, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(self.dropout(outputs.last_hidden_state))\n",
    "\n",
    "        if labels is not None:\n",
    "            # Replace -100 with a valid index (e.g., 0)\n",
    "            valid_labels = labels.clone()\n",
    "            valid_labels[labels == -100] = 0\n",
    "\n",
    "            # Compute CRF loss\n",
    "            loss = -self.crf(logits, valid_labels, mask=attention_mask.bool())\n",
    "            return loss\n",
    "        else:\n",
    "            # Decode CRF predictions\n",
    "            predictions = self.crf.decode(logits, mask=attention_mask.bool())\n",
    "            return predictions\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 5\n",
    "learning_rate = 3e-5\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Initialize the model\n",
    "improved_model = BertCRFNER(model_name=model_name, num_labels=num_labels)\n",
    "improved_model.to(device)\n",
    "\n",
    "# Optimizer and Scheduler setup\n",
    "optimizer = torch.optim.AdamW(improved_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Loss tracking\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "# Modify the training setup to support checkpoint resuming\n",
    "if resume_from_checkpoint:\n",
    "    # Load the checkpoint\n",
    "    # checkpoint = torch.load(resume_from_checkpoint)\n",
    "    with gzip.open(resume_from_checkpoint, 'rb') as f:\n",
    "\n",
    "        checkpoint = torch.load(f, map_location=device)\n",
    "\n",
    "    # Restore model state\n",
    "    improved_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    improved_model.to(device)\n",
    "\n",
    "    # Restore optimizer state\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # Restore scheduler state\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    # Set the starting epoch and potentially modify num_epochs\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "    # Optionally prepopulate loss lists with previous losses\n",
    "    train_losses = checkpoint.get('train_losses', [])\n",
    "    val_losses = checkpoint.get('val_losses', [])\n",
    "\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "# latest_checkpoint_path = os.path.join(google_drive_dir, \"latest_checkpoint.pt\")\n",
    "latest_checkpoint_path = os.path.join(google_drive_dir, \"latest_checkpoint.pt.gz\")\n",
    "# Training and Validation Loop\n",
    "for epoch in range(start_epoch,num_epochs):\n",
    "    # Training Phase\n",
    "    improved_model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = improved_model(input_ids, attention_mask, labels=labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(improved_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Average training loss for the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch + 1}: Training Loss = {avg_train_loss}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    improved_model.eval()\n",
    "    total_val_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            loss = improved_model(input_ids, attention_mask, labels=labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            preds = improved_model(input_ids, attention_mask)\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    # Average validation loss for the epoch\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch + 1}: Validation Loss = {avg_val_loss}\")\n",
    "\n",
    "    # torch.save({\n",
    "    #     'epoch': epoch + 1,\n",
    "    #     'model_state_dict': improved_model.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     'scheduler_state_dict': scheduler.state_dict()\n",
    "    # }, latest_checkpoint_path)\n",
    "\n",
    "    with gzip.open(latest_checkpoint_path, 'wb') as f:\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': improved_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict()\n",
    "        }, f)\n",
    "\n",
    "\n",
    "    print(f\"Checkpoint saved at: {latest_checkpoint_path}\")\n",
    "\n",
    "    # Decode and print classification report\n",
    "    decoded_predictions = []\n",
    "    decoded_labels = []\n",
    "    for preds, labels in zip(predictions, true_labels):\n",
    "        valid_preds = [p for p, l in zip(preds, labels) if l != -100]\n",
    "        valid_labels = [l for l in labels if l != -100]\n",
    "        decoded_predictions.append([label_list[p] for p in valid_preds])\n",
    "        decoded_labels.append([label_list[l] for l in valid_labels])\n",
    "\n",
    "    print(f\"Classification Report for Epoch {epoch + 1}:\")\n",
    "    print(classification_report(decoded_labels, decoded_predictions))\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', marker='o')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "loss_curve_path = os.path.join(google_drive_dir, 'loss_curve.png')\n",
    "plt.savefig(loss_curve_path)\n",
    "plt.close()\n",
    "\n",
    "print(\"Training completed. Checkpoints and loss curve saved in /content/checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mounted at /content/drive\n",
    "Using device: cuda\n",
    "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
    "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
    "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
    "You will be able to reuse this secret in all of your notebooks.\n",
    "Please note that authentication is recommended but still optional to access public models or datasets.\n",
    "  warnings.warn(\n",
    "README.md: 100%\n",
    " 4.45k/4.45k [00:00<00:00, 295kB/s]\n",
    "multiconer_v2.py: 100%\n",
    " 13.1k/13.1k [00:00<00:00, 968kB/s]\n",
    "0000.parquet: 100%\n",
    " 1.98M/1.98M [00:00<00:00, 35.9MB/s]\n",
    "0000.parquet: 100%\n",
    " 117k/117k [00:00<00:00, 6.63MB/s]\n",
    "0000.parquet: 100%\n",
    " 29.0M/29.0M [00:00<00:00, 196MB/s]\n",
    "Generating train split: 100%\n",
    " 16778/16778 [00:00<00:00, 146752.40 examples/s]\n",
    "Generating validation split: 100%\n",
    " 871/871 [00:00<00:00, 22792.85 examples/s]\n",
    "Generating test split: 100%\n",
    " 249980/249980 [00:00<00:00, 268050.39 examples/s]\n",
    "tokenizer_config.json: 100%\n",
    " 49.0/49.0 [00:00<00:00, 3.13kB/s]\n",
    "config.json: 100%\n",
    " 762/762 [00:00<00:00, 49.2kB/s]\n",
    "vocab.txt: 100%\n",
    " 213k/213k [00:00<00:00, 449kB/s]\n",
    "tokenizer.json: 100%\n",
    " 436k/436k [00:00<00:00, 29.1MB/s]\n",
    "Map: 100%\n",
    " 16778/16778 [00:05<00:00, 2146.63 examples/s]\n",
    "Map: 100%\n",
    " 871/871 [00:00<00:00, 2321.71 examples/s]\n",
    "Map: 100%\n",
    " 249980/249980 [01:16<00:00, 3728.98 examples/s]\n",
    "model.safetensors: 100%\n",
    " 1.34G/1.34G [00:08<00:00, 226MB/s]\n",
    "Training Epoch 1: 100%|██████████| 1049/1049 [22:37<00:00,  1.29s/it]\n",
    "Epoch 1: Training Loss = 117.65626361508728\n",
    "Evaluating: 100%|██████████| 55/55 [00:47<00:00,  1.17it/s]\n",
    "Epoch 1: Validation Loss = 65.69587013938211\n",
    "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, msg_start, len(result))\n",
    "Checkpoint saved at: /content/drive/My Drive/NER_checkpoints/latest_checkpoint.pt.gz\n",
    "Classification Report for Epoch 1:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "AerospaceManufacturer       0.38      0.60      0.46        10\n",
    "  AnatomicalStructure       0.65      0.76      0.70        17\n",
    "              ArtWork       0.50      0.38      0.43        13\n",
    "               Artist       0.74      0.83      0.78       212\n",
    "              Athlete       0.68      0.77      0.72        79\n",
    "      CarManufacturer       0.20      0.15      0.17        13\n",
    "               Cleric       0.45      0.33      0.38        15\n",
    "             Clothing       0.67      0.60      0.63        10\n",
    "              Disease       0.46      0.33      0.39        18\n",
    "                Drink       0.27      0.27      0.27        11\n",
    "             Facility       0.51      0.67      0.58        52\n",
    "                 Food       0.46      0.32      0.37        19\n",
    "      HumanSettlement       0.78      0.81      0.79       109\n",
    "     MedicalProcedure       0.40      0.62      0.48        13\n",
    "   Medication/Vaccine       0.68      0.72      0.70        18\n",
    "           MusicalGRP       0.66      0.62      0.64        37\n",
    "          MusicalWork       0.73      0.61      0.66        61\n",
    "                  ORG       0.43      0.54      0.48        78\n",
    "             OtherLOC       0.38      0.19      0.25        16\n",
    "             OtherPER       0.33      0.33      0.33        91\n",
    "            OtherPROD       0.38      0.39      0.38        49\n",
    "           Politician       0.39      0.53      0.45        53\n",
    "          PrivateCorp       0.00      0.00      0.00        11\n",
    "           PublicCorp       0.39      0.57      0.46        28\n",
    "            Scientist       0.29      0.33      0.31        15\n",
    "             Software       0.52      0.46      0.49        26\n",
    "            SportsGRP       0.86      0.78      0.82        41\n",
    "        SportsManager       0.69      0.69      0.69        16\n",
    "              Station       0.73      0.80      0.76        20\n",
    "              Symptom       0.33      0.20      0.25        10\n",
    "              Vehicle       0.39      0.35      0.37        20\n",
    "           VisualWork       0.75      0.75      0.75        61\n",
    "          WrittenWork       0.60      0.63      0.61        54\n",
    "\n",
    "            micro avg       0.58      0.61      0.60      1296\n",
    "            macro avg       0.51      0.51      0.50      1296\n",
    "         weighted avg       0.58      0.61      0.59      1296\n",
    "\n",
    "Training Epoch 2: 100%|██████████| 1049/1049 [22:35<00:00,  1.29s/it]\n",
    "Epoch 2: Training Loss = 53.443223507774796\n",
    "Evaluating: 100%|██████████| 55/55 [00:46<00:00,  1.17it/s]\n",
    "Epoch 2: Validation Loss = 60.406124184348364\n",
    "Checkpoint saved at: /content/drive/My Drive/NER_checkpoints/latest_checkpoint.pt.gz\n",
    "Classification Report for Epoch 2:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "AerospaceManufacturer       0.50      0.60      0.55        10\n",
    "  AnatomicalStructure       0.69      0.65      0.67        17\n",
    "              ArtWork       0.70      0.54      0.61        13\n",
    "               Artist       0.78      0.83      0.80       212\n",
    "              Athlete       0.70      0.80      0.75        79\n",
    "      CarManufacturer       0.47      0.54      0.50        13\n",
    "               Cleric       0.45      0.33      0.38        15\n",
    "             Clothing       0.45      0.50      0.48        10\n",
    "              Disease       0.44      0.44      0.44        18\n",
    "                Drink       0.50      0.55      0.52        11\n",
    "             Facility       0.68      0.69      0.69        52\n",
    "                 Food       0.64      0.47      0.55        19\n",
    "      HumanSettlement       0.82      0.89      0.85       109\n",
    "     MedicalProcedure       0.64      0.69      0.67        13\n",
    "   Medication/Vaccine       0.68      0.83      0.75        18\n",
    "           MusicalGRP       0.73      0.81      0.77        37\n",
    "          MusicalWork       0.74      0.74      0.74        61\n",
    "                  ORG       0.53      0.51      0.52        78\n",
    "             OtherLOC       0.78      0.44      0.56        16\n",
    "             OtherPER       0.45      0.49      0.47        91\n",
    "            OtherPROD       0.38      0.33      0.35        49\n",
    "           Politician       0.52      0.53      0.52        53\n",
    "          PrivateCorp       0.33      0.09      0.14        11\n",
    "           PublicCorp       0.44      0.50      0.47        28\n",
    "            Scientist       0.50      0.40      0.44        15\n",
    "             Software       0.68      0.50      0.58        26\n",
    "            SportsGRP       0.84      0.90      0.87        41\n",
    "        SportsManager       0.82      0.56      0.67        16\n",
    "              Station       0.94      0.75      0.83        20\n",
    "              Symptom       0.00      0.00      0.00        10\n",
    "              Vehicle       0.53      0.45      0.49        20\n",
    "           VisualWork       0.66      0.69      0.67        61\n",
    "          WrittenWork       0.67      0.72      0.70        54\n",
    "\n",
    "            micro avg       0.66      0.66      0.66      1296\n",
    "            macro avg       0.60      0.57      0.58      1296\n",
    "         weighted avg       0.65      0.66      0.65      1296\n",
    "\n",
    "Training Epoch 3: 100%|██████████| 1049/1049 [22:39<00:00,  1.30s/it]\n",
    "Epoch 3: Training Loss = 33.29585610126972\n",
    "Evaluating: 100%|██████████| 55/55 [00:47<00:00,  1.17it/s]\n",
    "Epoch 3: Validation Loss = 60.80253365256569\n",
    "Checkpoint saved at: /content/drive/My Drive/NER_checkpoints/latest_checkpoint.pt.gz\n",
    "Classification Report for Epoch 3:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "AerospaceManufacturer       0.57      0.80      0.67        10\n",
    "  AnatomicalStructure       0.69      0.65      0.67        17\n",
    "              ArtWork       0.50      0.23      0.32        13\n",
    "               Artist       0.73      0.87      0.80       212\n",
    "              Athlete       0.76      0.78      0.77        79\n",
    "      CarManufacturer       0.54      0.54      0.54        13\n",
    "               Cleric       0.50      0.33      0.40        15\n",
    "             Clothing       0.64      0.70      0.67        10\n",
    "              Disease       0.50      0.61      0.55        18\n",
    "                Drink       0.60      0.82      0.69        11\n",
    "             Facility       0.64      0.79      0.71        52\n",
    "                 Food       0.83      0.53      0.65        19\n",
    "      HumanSettlement       0.80      0.90      0.85       109\n",
    "     MedicalProcedure       0.57      0.62      0.59        13\n",
    "   Medication/Vaccine       0.71      0.83      0.77        18\n",
    "           MusicalGRP       0.86      0.81      0.83        37\n",
    "          MusicalWork       0.69      0.77      0.73        61\n",
    "                  ORG       0.63      0.54      0.58        78\n",
    "             OtherLOC       0.62      0.31      0.42        16\n",
    "             OtherPER       0.51      0.44      0.47        91\n",
    "            OtherPROD       0.50      0.49      0.49        49\n",
    "           Politician       0.52      0.57      0.54        53\n",
    "          PrivateCorp       0.20      0.09      0.13        11\n",
    "           PublicCorp       0.59      0.57      0.58        28\n",
    "            Scientist       0.43      0.40      0.41        15\n",
    "             Software       0.64      0.62      0.63        26\n",
    "            SportsGRP       0.86      0.93      0.89        41\n",
    "        SportsManager       0.85      0.69      0.76        16\n",
    "              Station       0.80      0.80      0.80        20\n",
    "              Symptom       0.60      0.30      0.40        10\n",
    "              Vehicle       0.56      0.50      0.53        20\n",
    "           VisualWork       0.62      0.77      0.69        61\n",
    "          WrittenWork       0.74      0.69      0.71        54\n",
    "\n",
    "            micro avg       0.67      0.69      0.68      1296\n",
    "            macro avg       0.63      0.61      0.61      1296\n",
    "         weighted avg       0.67      0.69      0.67      1296\n",
    "\n",
    "Training Epoch 4: 100%|██████████| 1049/1049 [22:35<00:00,  1.29s/it]\n",
    "Epoch 4: Training Loss = 21.13793125279866\n",
    "Evaluating: 100%|██████████| 55/55 [00:47<00:00,  1.17it/s]\n",
    "Epoch 4: Validation Loss = 69.80272369384765\n",
    "Checkpoint saved at: /content/drive/My Drive/NER_checkpoints/latest_checkpoint.pt.gz\n",
    "Classification Report for Epoch 4:\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "AerospaceManufacturer       0.62      0.80      0.70        10\n",
    "  AnatomicalStructure       0.67      0.71      0.69        17\n",
    "              ArtWork       0.46      0.46      0.46        13\n",
    "               Artist       0.81      0.79      0.80       212\n",
    "              Athlete       0.81      0.75      0.78        79\n",
    "      CarManufacturer       0.62      0.62      0.62        13\n",
    "               Cleric       0.50      0.40      0.44        15\n",
    "             Clothing       0.67      0.80      0.73        10\n",
    "              Disease       0.56      0.50      0.53        18\n",
    "                Drink       0.58      0.64      0.61        11\n",
    "             Facility       0.67      0.79      0.73        52\n",
    "                 Food       0.50      0.58      0.54        19\n",
    "      HumanSettlement       0.81      0.87      0.84       109\n",
    "     MedicalProcedure       0.57      0.62      0.59        13\n",
    "   Medication/Vaccine       0.71      0.83      0.77        18\n",
    "           MusicalGRP       0.86      0.81      0.83        37\n",
    "          MusicalWork       0.76      0.82      0.79        61\n",
    "                  ORG       0.58      0.58      0.58        78\n",
    "             OtherLOC       0.42      0.31      0.36        16\n",
    "             OtherPER       0.47      0.68      0.56        91\n",
    "            OtherPROD       0.43      0.47      0.45        49\n",
    "           Politician       0.59      0.51      0.55        53\n",
    "          PrivateCorp       0.33      0.18      0.24        11\n",
    "           PublicCorp       0.58      0.54      0.56        28\n",
    "            Scientist       0.36      0.33      0.34        15\n",
    "             Software       0.62      0.50      0.55        26\n",
    "            SportsGRP       0.80      0.90      0.85        41\n",
    "        SportsManager       0.67      0.62      0.65        16\n",
    "              Station       0.83      0.75      0.79        20\n",
    "              Symptom       0.67      0.60      0.63        10\n",
    "              Vehicle       0.48      0.50      0.49        20\n",
    "           VisualWork       0.74      0.75      0.75        61\n",
    "          WrittenWork       0.63      0.70      0.67        54\n",
    "\n",
    "            micro avg       0.67      0.69      0.68      1296\n",
    "            macro avg       0.62      0.63      0.62      1296\n",
    "         weighted avg       0.67      0.69      0.68      1296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Reduce the test dataset size (e.g., 10% of the original size)\n",
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=42).select(range(int(len(dataset[\"test\"]) * 0.1)))\n",
    "\n",
    "# Reapply tokenization to the reduced test dataset\n",
    "tokenized_datasets[\"test\"] = dataset[\"test\"].map(tokenize_and_align_labels_with_crf, batched=True)\n",
    "tokenized_datasets[\"test\"] = tokenized_datasets[\"test\"].remove_columns([\"id\", \"sample_id\", \"tokens\", \"ner_tags\", \"ner_tags_index\"])\n",
    "tokenized_datasets[\"test\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Recreate the test dataloader with the smaller dataset\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=16, collate_fn=data_collator)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "improved_model.eval()\n",
    "\n",
    "# Lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Disable gradient computation during testing\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Get predictions\n",
    "        preds = improved_model(input_ids, attention_mask)\n",
    "\n",
    "        # Extend predictions and true labels\n",
    "        all_predictions.extend(preds)\n",
    "        all_true_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "# Decode predictions and true labels\n",
    "decoded_predictions = []\n",
    "decoded_labels = []\n",
    "for preds, labels in zip(all_predictions, all_true_labels):\n",
    "    valid_preds = [p for p, l in zip(preds, labels) if l != -100]\n",
    "    valid_labels = [l for l in labels if l != -100]\n",
    "    decoded_predictions.append([label_list[p] for p in valid_preds])\n",
    "    decoded_labels.append([label_list[l] for l in valid_labels])\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report on Test Data (10% subset):\")\n",
    "print(classification_report(decoded_labels, decoded_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map: 100%\n",
    " 24998/24998 [00:09<00:00, 3808.53 examples/s]\n",
    "Testing: 100%|██████████| 1563/1563 [11:00<00:00,  2.37it/s]\n",
    "Classification Report on Test Data (10% subset):\n",
    "                       precision    recall  f1-score   support\n",
    "\n",
    "AerospaceManufacturer       0.35      0.63      0.45        82\n",
    "  AnatomicalStructure       0.65      0.71      0.68       567\n",
    "              ArtWork       0.40      0.56      0.46       149\n",
    "               Artist       0.71      0.81      0.76      5696\n",
    "              Athlete       0.77      0.76      0.77      2676\n",
    "      CarManufacturer       0.58      0.60      0.59       297\n",
    "               Cleric       0.50      0.49      0.50       489\n",
    "             Clothing       0.58      0.61      0.60       224\n",
    "              Disease       0.65      0.63      0.64       566\n",
    "                Drink       0.52      0.55      0.53       203\n",
    "             Facility       0.62      0.67      0.65      1605\n",
    "                 Food       0.50      0.52      0.51       545\n",
    "      HumanSettlement       0.86      0.87      0.87      4205\n",
    "     MedicalProcedure       0.60      0.59      0.59       383\n",
    "   Medication/Vaccine       0.68      0.71      0.69       586\n",
    "           MusicalGRP       0.63      0.67      0.65      1248\n",
    "          MusicalWork       0.66      0.72      0.69      1525\n",
    "                  ORG       0.54      0.62      0.58      2307\n",
    "             OtherLOC       0.48      0.48      0.48       472\n",
    "             OtherPER       0.38      0.44      0.41      2170\n",
    "            OtherPROD       0.46      0.48      0.47      1165\n",
    "           Politician       0.52      0.53      0.53      1626\n",
    "          PrivateCorp       0.17      0.32      0.22       111\n",
    "           PublicCorp       0.54      0.53      0.53       664\n",
    "            Scientist       0.44      0.34      0.38       482\n",
    "             Software       0.65      0.69      0.67       883\n",
    "            SportsGRP       0.78      0.82      0.80      1280\n",
    "        SportsManager       0.53      0.53      0.53       509\n",
    "              Station       0.77      0.78      0.77       617\n",
    "              Symptom       0.38      0.52      0.44       183\n",
    "              Vehicle       0.46      0.51      0.49       618\n",
    "           VisualWork       0.68      0.73      0.71      2000\n",
    "          WrittenWork       0.65      0.66      0.66      1662\n",
    "\n",
    "            micro avg       0.64      0.68      0.66     37795\n",
    "            macro avg       0.57      0.61      0.58     37795\n",
    "         weighted avg       0.64      0.68      0.66     37795"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
